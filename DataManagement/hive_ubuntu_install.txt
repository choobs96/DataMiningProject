sudo usermod -aG sudo hduser


sudo apt-get update
sudo apt install openjdk-8-jdk

Adding Dedicated Hadoop user:
sudo addgroup hadoop
sudo adduser --ingroup hadoop hduser

Install ssh
sudo apt-get install ssh

su hduser
ssh-keygen -t rsa -P

Below command adds the newly created key to the list of authorized keys so that Hadoop can use ssh without prompting for a password

cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys
ssh localhost

download Hadoop
wget https://www-us.apache.org/dist/hadoop/common/hadoop-2.8.5/hadoop-2.8.5.tar.gz

tar xvzf hadoop-2.8.5.tar.gz

sudo mv hadoop-2.8.5 /usr/local/hadoop


update-alternatives --config java
/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java

1) Edit .bashrc file
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_INSTALL=/usr/local/hadoop
export PATH=$PATH:$HADOOP_INSTALL/bin
export PATH=$PATH:$HADOOP_INSTALL/sbin
export HADOOP_MAPRED_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_HOME=$HADOOP_INSTALL
export HADOOP_HDFS_HOME=$HADOOP_INSTALL
export YARN_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_INSTALL/lib"


2. /usr/local/hadoop/etc/hadoop/hadoop-env.sh
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

3. /usr/local/hadoop/etc/hadoop/core-site.xml:
sudo mkdir -p /app/hadoop/tmp
sudo chown hduser:hadoop /app/hadoop/tmp
sudo nano /usr/local/hadoop/etc/hadoop/core-site.xml


<configuration>
<property>
 <name>hadoop.tmp.dir</name>
 <value>/app/hadoop/tmp</value>
 <description>A base for other temporary directories.</description>
</property>

<property>
 <name>fs.default.name</name>
 <value>hdfs://localhost:54310</value>
 <description>The name of the default file system.... </description>
</property>
</configuration>


4. /usr/local/hadoop/etc/hadoop/mapred-site.xml
sudo cp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template /usr/local/hadoop/etc/hadoop/mapred-site.xml
sudo nano /usr/local/hadoop/etc/hadoop/mapred-site.xml

<configuration>
 <property>
  <name>mapred.job.tracker</name>
  <value>localhost:54311</value>
  <description>The host and port that the Mapreduce job tracker runs at...</description>
 </property>
</configuration>


5. /usr/local/hadoop/etc/hadoop/hdfs-site.xml
sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode
sudo mkdir -p /usr/local/hadoo_store/hdfs/datanode
sudo chown -R hduser:hadoop /usr/local/hadoop_store
sudo nano /usr/local/hadoop/etc/hadoop/hdfs-site.xml

<configuration>
 <property>
  <name>dfs.replication</name>
  <value>1</value>
  <description>Default block replication...</description>
 </property>
 <property>
  <name>dfs.namenode.name.dir</name>
  <value>file:/usr/local/hadoop_store/hdfs/namenode</value>
 </property>
 <property>
  <name>dfs.datanode.data.dir</name>
  <value>file:/usr/local/hadoop_store/hdfs/datanode</value>
 </property>
</configuration>



Format the New Hadoop Filesystem
hadoop namenode -format


sudo chmod -R o+w /usr/local/hadoop_store/hdfs
start-all.sh
jps

stop-all.sh




### HIVE

1. Download apache hive
   wget https://www-us.apache.org/dist/hive/hive-2.3.6/apache-hive-2.3.6-bin.tar.gz

2. Extract to /usr/local
   sudo tar xvzf apache-hive-2.3.6-bin.tar.gz -C /usr/local

3. Edit the environment variables
sudo nano .bashrc

export HIVE_HOME=/usr/local/apache-hive-2.3.6-bin
export HIVE_CONF_DIR=/usr/local/apache-hive-2.3.6-bin/conf
export PATH=$HIVE_HOME/bin:$PATH
export CLASSPATH=$CLASSPATH:/usr/local/hadoop/lib/*:.
export CLASSPATH=$CLASSPATH:/usr/local/apache-hive-2.3.6-bin/lib/*:.


4. (hduser)
hdfs dfs -mkdir /user 
hdfs dfs -mkdir /user/hive
hdfs dfs -mkdir /user/hive/warehouse
hdfs dfs -mkdir /user/hive/tmp
hdfs dfs -mkdir /tmp
hdfs dfs -chmod g+w /tmp
hdfs dfs -chmod g+w /user/hive/warehouse
hdfs dfs -chmod g+w /user/hive/tmp

5. 
cd $HIVE_HOME/conf
sudo cp hive-env.sh.template hive-env.sh

sudo nano hive-env.sh
export HADOOP_HOME=/usr/local/hadoop



### Apache Derby

wget http://archive.apache.org/dist/db/derby/db-derby-10.13.1.1/db-derby-10.13.1.1-bin.tar.gz
sudo tar xvzf db-derby-10.13.1.1-bin.tar.gz -C /usr/local

sudo nano .bashrc
export DERBY_HOME=/usr/local/db-derby-10.13.1.1-bin
export PATH=$PATH:$DERBY_HOME/bin
export CLASSPATH=$CLASSPATH:$DERBY_HOME/lib/derby.jar:$DERBY_HOME/lib/derbytools.jar

sudo mkdir $DERBY_HOME/data

1. Editing the hive-site.xml file
cd $HIVE_HOME/conf
sudo cp hive-default.xml.template hive-site.xml

sudo nano hive-site.xml
<property><name>javax.jdo.option.ConnectionURL</name><value>jdbc:derby:;databaseName=metastore_db;create=true</value><description>JDBC connect string for a JDBC metastore.To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.
</description>
</property>


2. 
sudo nano jpox.properties

javax.jdo.PersistenceManagerFactoryClass =org.jpox.PersistenceManagerFactoryImpl
org.jpox.autoCreateSchema = false
org.jpox.validateTables = false
org.jpox.validateColumns = false
org.jpox.validateConstraints = false
org.jpox.storeManagerType = rdbms
org.jpox.autoCreateSchema = true
org.jpox.autoStartMechanismMode = checked
org.jpox.transactionIsolation = read_committed
javax.jdo.option.DetachAllOnCommit = true
javax.jdo.option.NontransactionalRead = truejavax.jdo.option.ConnectionDriverName = org.apache.derby.jdbc.ClientDriverjavax.jdo.option.ConnectionURL = jdbc:derby://hadoop1:1527/metastore_db;create = truejavax.jdo.option.ConnectionUserName = APPjavax.jdo.option.ConnectionPassword = mine


3. Metastore schema initialization

#cd $HIVE_HOME/bin
#sudo ./schematool -dbType derby -initSchema

?? if Schema initialization FAILED! Metastore state would be inconsistent !!
cd $HIVE_HOME/bin
sudo mv metastore_db metastore_db.tmp
??

su hduser
cd $HIVE_HOME/bin
sudo ./schematool -dbType derby -initSchema

4. 
sudo nano /usr/local/apache-hive-2.3.6-bin/conf/hive-site.xml

<property>
    <name>system:java.io.tmpdir</name>
    <value>/tmp/hive/java</value>
  </property>
  <property>
    <name>system:user.name</name>
    <value>${user.name}</value>
</property>

<property>
  <name>hive.exec.local.scratchdir</name>
  <value>/tmp/myDir</value>
  <description>Local scratch space for Hive jobs</description>
</property>

<property>
  <name>hive.downloaded.resources.dir</name>
  <value>/user/hive/tmp/${hive.session.id}_resources</value>  
  <description>Temporary local directory for added resources in the remove file system.</description>
</property>

<value>/home/hduser/hive/tmp/${hive.session.id}_resources</value>
